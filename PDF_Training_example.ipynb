{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNl/jQmaS8dxB1r7FJ13AFt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heather-Herbert/colab/blob/main/PDF_Training_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zEop_jiP97T"
      },
      "outputs": [],
      "source": [
        "# Google Colab Notebook Template for Fine-tuning an LLM with bitsandbytes\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install transformers datasets peft pypdf sentencepiece bitsandbytes accelerate huggingface_hub\n",
        "\n",
        "# Import libraries\n",
        "from pypdf import PdfReader\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from huggingface_hub import notebook_login, HfApi\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Log in to Hugging Face\n",
        "notebook_login()\n",
        "\n",
        "# Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "pdf_text = extract_text_from_pdf('/content/drive/My Drive/subdirectory/your_pdf.pdf')\n",
        "\n",
        "# Prepare dataset\n",
        "data = {\"text\": pdf_text.split('\\n\\n')}  # Split into smaller chunks\n",
        "dataset = Dataset.from_dict(data)\n",
        "\n",
        "# Load Qwen 2.5-7B model and tokenizer with bitsandbytes for quantization\n",
        "model_name = \"Qwen/Qwen2.5-7B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map=\"auto\", quantization_config={\"load_in_4bit\": True})\n",
        "\n",
        "# PEFT configuration for LoRA\n",
        "lora_config = LoraConfig(r=8, lora_alpha=16, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save and push the fine-tuned model to Hugging Face Hub\n",
        "model.push_to_hub(\"your-huggingface-username/your-model-repo\")\n",
        "tokenizer.push_to_hub(\"your-huggingface-username/your-model-repo\")\n"
      ]
    }
  ]
}